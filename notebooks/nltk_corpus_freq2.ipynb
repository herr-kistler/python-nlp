{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "from nltk.book, compute vocab of sent1...sent8 using list addition and the set and sorted operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.book import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['!',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '1',\n",
       " '25',\n",
       " '29',\n",
       " '61',\n",
       " ':',\n",
       " '[',\n",
       " ']',\n",
       " 'a',\n",
       " 'and',\n",
       " 'arthur',\n",
       " 'as',\n",
       " 'attrac',\n",
       " 'been',\n",
       " 'beginning',\n",
       " 'board',\n",
       " 'call',\n",
       " 'citizens',\n",
       " 'clop',\n",
       " 'created',\n",
       " 'dashwood',\n",
       " 'director',\n",
       " 'discreet',\n",
       " 'earth',\n",
       " 'encounters',\n",
       " 'family',\n",
       " 'fellow',\n",
       " 'for',\n",
       " 'god',\n",
       " 'had',\n",
       " 'have',\n",
       " 'heaven',\n",
       " 'house',\n",
       " 'i',\n",
       " 'in',\n",
       " 'ishmael',\n",
       " 'join',\n",
       " 'king',\n",
       " 'lady',\n",
       " 'lol',\n",
       " 'long',\n",
       " 'male',\n",
       " 'me',\n",
       " 'nonexecutive',\n",
       " 'nov.',\n",
       " 'of',\n",
       " 'old',\n",
       " 'older',\n",
       " 'people',\n",
       " 'pierre',\n",
       " 'pming',\n",
       " 'problem',\n",
       " 'representatives',\n",
       " 'scene',\n",
       " 'seeks',\n",
       " 'senate',\n",
       " 'settled',\n",
       " 'sexy',\n",
       " 'single',\n",
       " 'sussex',\n",
       " 'the',\n",
       " 'there',\n",
       " 'to',\n",
       " 'vinken',\n",
       " 'whoa',\n",
       " 'will',\n",
       " 'wind',\n",
       " 'with',\n",
       " 'years']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#instead of list addition, which is more inefficient, I am using sets\n",
    "\n",
    "running = set(sent1)\n",
    "running.update(sent2, sent3, sent4, sent5, sent6, sent7, sent8)\n",
    "running = set([w.lower() for w in running])\n",
    "sorted(list(running))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function called vocab_size(text) that has a single parameter for the text, and which returns the (normalized) vocabulary size of that text. Show that it works by applying it to the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "def vocab_size(text):\n",
    "    result = set([w.lower() for w in text.words()])\n",
    "    return len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49815"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "vocab_size(nltk.corpus.brown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to print the 50 most frequent bigrams of the Penn Tree bank corpus, omitting bigrams that contain stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def freq_bigrams(text, language, num_bigrams):\n",
    "    bigrams = nltk.bigrams([w.lower() for w in text.words()])\n",
    "    freq_dist = nltk.FreqDist(bigrams)\n",
    "    keys = freq_dist.keys()\n",
    "    stopwords = nltk.corpus.stopwords.words(language)\n",
    "    new = []\n",
    "    for bigram in keys:\n",
    "        if bigram[0] not in stopwords:\n",
    "            if bigram[1] not in stopwords:\n",
    "                new.append(bigram)\n",
    "    return new[:num_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('pierre', 'vinken'),\n",
       " ('vinken', ','),\n",
       " (',', '61'),\n",
       " ('61', 'years'),\n",
       " ('years', 'old'),\n",
       " ('old', ','),\n",
       " ('nonexecutive', 'director'),\n",
       " ('director', 'nov.'),\n",
       " ('nov.', '29'),\n",
       " ('29', '.'),\n",
       " ('.', 'mr.'),\n",
       " ('mr.', 'vinken'),\n",
       " ('elsevier', 'n.v.'),\n",
       " ('n.v.', ','),\n",
       " ('dutch', 'publishing'),\n",
       " ('publishing', 'group'),\n",
       " ('group', '.'),\n",
       " ('.', 'rudolph'),\n",
       " ('rudolph', 'agnew'),\n",
       " ('agnew', ','),\n",
       " (',', '55'),\n",
       " ('55', 'years'),\n",
       " ('former', 'chairman'),\n",
       " ('consolidated', 'gold'),\n",
       " ('gold', 'fields'),\n",
       " ('fields', 'plc'),\n",
       " ('plc', ','),\n",
       " ('named', '*-1'),\n",
       " ('british', 'industrial'),\n",
       " ('industrial', 'conglomerate'),\n",
       " ('conglomerate', '.'),\n",
       " ('used', '*'),\n",
       " ('*', '*'),\n",
       " ('make', 'kent'),\n",
       " ('kent', 'cigarette'),\n",
       " ('cigarette', 'filters'),\n",
       " ('high', 'percentage'),\n",
       " ('cancer', 'deaths'),\n",
       " ('deaths', 'among'),\n",
       " ('workers', 'exposed'),\n",
       " ('exposed', '*'),\n",
       " ('30', 'years'),\n",
       " ('years', 'ago'),\n",
       " ('ago', ','),\n",
       " (',', 'researchers'),\n",
       " ('researchers', 'reported'),\n",
       " ('reported', '0'),\n",
       " ('0', '*t*-1'),\n",
       " ('*t*-1', '.'),\n",
       " ('asbestos', 'fiber')]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "freq_bigrams(nltk.corpus.treebank, 'English', 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the file science.txt from blackboard, read it in as a corpus, tokenize it, and print a list of all wh-word types that occur along with their frequencies. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading science.txt: Package 'science.txt' not found\n",
      "[nltk_data]     in index\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mscience.txt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('science.txt')\n  \u001b[0m\n  Searched in:\n    - '/Users/tylerkistler/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/tylerkistler/anaconda/nltk_data'\n    - '/Users/tylerkistler/anaconda/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-b9eb4e2f1dff>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mwordlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileids\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mnewcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordlists\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'science.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ling571/science.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m#read in text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.6/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    671\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'*'\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\n%s\\n%s\\n%s\\n'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mscience.txt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('science.txt')\n  \u001b[0m\n  Searched in:\n    - '/Users/tylerkistler/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/Users/tylerkistler/anaconda/nltk_data'\n    - '/Users/tylerkistler/anaconda/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "nltk.download('science.txt')\n",
    "\n",
    "corpus_root = '/Users/tylerkistler/ling571/'\n",
    "wordlists = PlaintextCorpusReader(corpus_root, '.*')\n",
    "wordlists.fileids()\n",
    "newcorpus = wordlists.words('science.txt')\n",
    "path = nltk.data.find('ling571/science.txt')\n",
    "\n",
    "#read in text\n",
    "t = open(path, 'rU')\n",
    "raw = t.read()\n",
    "\n",
    "#tokenize text\n",
    "tokens = word_tokenize(raw)\n",
    "\n",
    "#pulls and prints all words that start with wh\n",
    "wh_words = [word for word in tokens if word.startswith('wh')]\n",
    "\n",
    "#sorts and prints list\n",
    "wh_words.sort()\n",
    "print (wh_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
